{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an ML Platform with MLflow\n",
    "The purpose of this notebook is to walk through all the steps for managing a machine learning model in MLFlow from data ingestion to model inference and deployment.\n",
    "\n",
    "### Steps\n",
    "The notebook will cover the following steps:\n",
    "1. Data Ingestion\n",
    "2. Data Preprocessing\n",
    "3. Parameter Tuning\n",
    "4. Model Training\n",
    "5. Model Deployment\n",
    "6. Model Inference\n",
    "\n",
    "\n",
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>interaction student teacher teachers come teac...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teaching not upto mark focus topper student ha...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teacher us instructional strategy ineffective ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teacher conveys content inaccuracy contribute ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boring dull lecture</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  interaction student teacher teachers come teac...     -1\n",
       "1  teaching not upto mark focus topper student ha...     -1\n",
       "2  teacher us instructional strategy ineffective ...     -1\n",
       "3  teacher conveys content inaccuracy contribute ...     -1\n",
       "4                                boring dull lecture     -1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/feedback.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "In this step we apply the following steps to ensure the dataset is ready for training.\n",
    "* Drop rows with null values\n",
    "* Remove special characters\n",
    "* Remove stop words\n",
    "* Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess data for model training\n",
    "    \"\"\"\n",
    "    # Drop rows with missing values\n",
    "    df = df.dropna()\n",
    "    # Lowercasing\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # Remove special characters and numbers\n",
    "    df['text'] = df['text'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    # Create label to category mapping\n",
    "    label2cat = {1: 'positive', 0: 'neutral', -1: 'negative'}\n",
    "    cat2label = {cat: label for label, cat in label2cat.items()}\n",
    "    df['label'] = df['label'].map(cat2label)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocess_data(df)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning\n",
    "In this section we run several parallel runs and compare them given a defined performance metric. In our case we use accuracy, but this can obviously be easily changed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import random\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def run_tuning(tracking_uri: str,\n",
    "               experiment_name: str,\n",
    "               run_name: str = None,\n",
    "               model_params: dict = None,\n",
    "               tokenizer_params: dict = None,\n",
    "               data_params: dict = None,\n",
    "               df: pd.DataFrame = None\n",
    "    ):\n",
    "\n",
    "    \"\"\"\n",
    "    Runs trainig as MLFlow experiment\n",
    "\n",
    "    Args:\n",
    "        tracking_uri (str): URI of MLFlow tracking server\n",
    "        experiment_name (str): Name of experiment\n",
    "        run_name (str): Name of current mlflow run\n",
    "        model_params (dict): Dictionary of model parameters\n",
    "        tokenizer_params (dict): Dictionary of tokenizer parameters\n",
    "        data_params (dict): Dictionary of data parameters\n",
    "        df (pd.DataFrame): Dataframe containing text and label columns\n",
    "    \n",
    "    Returns:\n",
    "        results (dict): dictionary of results\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    # Split dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        df['text'].tolist(), \n",
    "        df['label'].tolist(), \n",
    "        test_size=data_params['test_size'], \n",
    "        random_state=data_params['random_state']\n",
    "    )\n",
    "    # Generate current timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    # Start MLFlow run\n",
    "    with mlflow.start_run(nested=True, run_name=f'{run_name}:{timestamp}') as run:\n",
    "        # Vectorize text\n",
    "        cv = CountVectorizer(\n",
    "            max_features=tokenizer_params['max_features'], \n",
    "            ngram_range=tokenizer_params['ngram_range'], \n",
    "            max_df=tokenizer_params['max_df'],\n",
    "            min_df=tokenizer_params['min_df'],\n",
    "        )\n",
    "        X = cv.fit_transform(x_train)\n",
    "        # Initialize and fit model\n",
    "        model = MultinomialNB(alpha=model_params['alpha'], fit_prior=model_params['fit_prior'])\n",
    "        model.fit(X.toarray(), y_train)\n",
    "        # Generate predicitons on test set\n",
    "        predictions = model.predict(cv.transform(x_test).toarray())\n",
    "        # Get test metrics\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        precision = precision_score(y_test, predictions, average='weighted')\n",
    "        recall = recall_score(y_test, predictions, average='weighted')\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params(model_params)\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        })\n",
    "        # Log model and tokenizer artifacts\n",
    "        mlflow.sklearn.log_model(\n",
    "            cv,\n",
    "            \"tokenizer\"\n",
    "        )\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            \"student-model\"\n",
    "        )\n",
    "        \n",
    "    return {\"accuracy\": accuracy, \"model\": model, \"tokenizer\": cv, \"model_params\": model_params, \"tokenizer_params\": tokenizer_params, \"run_id\": run.info.run_id}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jokes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
