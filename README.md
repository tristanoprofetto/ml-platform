# Machine Learning Platform with MLflow

## Project Overview
This Machine Learning Platform is designed as a template for training, tuning, and serving machine learning models using Docker containers. For the focus is on experimentation and running workloads from your local machine. The project leverages the open-source [MLflow framework](https://mlflow.org/docs/latest/index.html) for managing the machine learning lifecycle, including experimentation, reproducibility, and deployment.

## Features
- **Automation**: Leverages Bash scripts to streamline the definition of variables and execution of ML workflows
- **MLflow Integration**: Utilizes MLflow for managing the machine learning lifecycle, including experimentation, reproducibility, and deployment.
- **Parameter Tuning at Scale**: MLflow runs are submitted as seperate tasks to the ProcessPoolExecutor. Since each task is run in a separate process, they can execute in parallel.
- **Dockerized Components**: Separate Docker containers for model training, parameter tuning, and serving the model, ensuring modularity and scalability.
- **Logging System**: logging for visibility across running services
- **Error Handling**: custom defined exceptions for identifying where code breaks
- **Dataset Structure**: The dataset consists of two columns - 'text' (student feedback) and 'labels' (sentiment labels). Some data was generated by ChatGPT and some was found here: https://github.com/gautamgc17/Student-Feedback-Sentiment-Analysis/blob/main/feedback1.json


### Prerequisites
Be sure to have installed the following on your machine.
- Python 3.x
- Docker
- MLflow

### Run the MLflow Server
Run the folllowing to command to start the mlflow tracking UI on your local machine:
```
mlflow server --host=$MLFLOW_SERVER_HOST --port=$MLFLOW_SERVER_PORT --serve-artifacts
```
Be sure to use your IP address as the host server so that your docker containers can communicate with the MLflow tracking server. You can print your local IP with the following command:
```sh
ifconfig | grep inet
```

### ML Workloads
There are three primary workloads we can execute:
1. Parameter Tuning: running multiple mlflow runs in parallel and selecting the best result
2. Model Training: running model training
3. Deployment and Inference: serving the model to make predictions

### How to Run ML Workloads
1. Docker Containers (Recommended): run workloads as isolated snapshots of your code
2. Command-line: run the scripts directly from the console

### 01 - Getting Started - Running with Docker
1. **Clone the Repository**
   ```sh
   git clone https://github.com/tristanoprofetto/ml-platform
   ```

2. **Define Environment Variables**
   Navigate to the automation folder and define the required environment variables in ./automation/variables.sh


3. **Run ML Workflows**
   Execute training, tuning, or deployment jobs by executing any of these commands
   * Model Training:
   ```sh
   bash ./automation/execute.sh train
   ```
   * Parameter Tuning:
   ```sh
   bash ./automation/execute.sh tune
   ```
   * Deploy
   ```sh
   bash ./automation/execute.sh deploy
   ```
   * End-to-End (coming soon)

### 02 - Getting Started - Executing Scripts Locally
##### Finding the best Model Parameters
1. Run parameter tuning directly:
```sh
python3 ./steps/tune.py --tracking_uri=$MLFLOW_TRACKING_URI --experiment_name=$MLFLOW_EXPERIMENT_NAME --run_name=$MLFLOW_RUN_NAME
```
2. Navigate to MLFlow Server Check and Compare MLFlow runs through the Tracking UI

##### Training the Model
1. Set model, tokenizer, and data parameters for running the training job in the conf.ini file.
2. Run the training script.
   ```sh
   python ./steps/train.py --tracking_uri=$MLFLOW_TRACKING_URI --experiment_name=$MLFLOW_EXPERIMENT_NAME --run_name=$MLFLOW_RUN_NAME
   ```
3. Navigate to the MLFlow Tracking UI to visualize results

##### Serving the Model
1. Make sure to copy the run_id for the specific model you want to deploy
2. Build the docker image by running:
   ```sh
   docker build -t $SERVE_IMAGE_TAG -f ./dockerfiles/serve/Dockerfile .
   ```
3. Run the container:
   ```sh
   docker run -p 8000:8000 -e TRACKING_URI=$MLFLOW_TRACKING_URI -e RUN_ID=$SERVE_RUN_ID $SERVE_IMAGE_TAG
   ```
4. Run the predict.py script to make inferences on the running container. Add your own inputs you want to test.

## High-Level System Architecture (ideally)
1. UI Layer: MLFlow tracking UI for visualizing and comparing experiments, registered models, runs, artifacts, metrics and more.
2. Application Layer: for managing the orchestration of ML workloads and networking between services.
3. Data Layer: manages data for running ML workloads
4. Logging and Monitoring: ensures visiblity of health and performance across services
5. Testing: ensures reliability and functionality of system components and their interactions

## Additional Considerations
1. Scalability: connecting to cloud platforms for handling larger datasets, and running more intensive workloads.
2. Security: implement robust security measures, especially when handling sensitive datasets.
3. CI/CD: automate the testing and deployment of services in a production environment.
4. Documentation and Support: write more comprehensive documentation and potentially support channels.

### Code Structure
Here is a quick breakdown of the code structure
* automation: automate the execution of ML workflows with bash scripts
* data: sample dataset for running training and tuning
* dockerfiles: required dockerfiles for building the images
* exceptions: custom defined exceptions for better error handling
* logger: custom Python logger
* steps: the required modules that execute our machine learning workflows
* tests: functional testing
* app.py: model server file

