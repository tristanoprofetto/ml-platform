# Machine Learning Platform with MLflow

## Project Overview
This Machine Learning Platform is designed as a template for training, tuning, and serving machine learning models using Docker containers. The project leverages the open-source MLflow framework, focusing on a Naive Bayes classifier from the scikit-learn library to classify text data. It's specifically tailored for analyzing student feedback 'reflections', classifying them into one of three categories: negative, neutral, or positive.

## Features
- **MLflow Integration**: Utilizes MLflow for managing the machine learning lifecycle, including experimentation, reproducibility, and deployment.
- **Dockerized Components**: Separate Docker containers for model training, parameter tuning, and serving the model, ensuring modularity and scalability.
- **Naive Bayes Classifier**: Employs a Naive Bayes classifier from scikit-learn for text classification.
- **Sentiment Analysis**: Classifies student feedback into negative, neutral, and positive categories.
- **Dataset Structure**: The dataset consists of two columns - 'text' (student feedback) and 'labels' (sentiment labels). Some data was generated by ChatGPT and some was found here: https://github.com/gautamgc17/Student-Feedback-Sentiment-Analysis/blob/main/feedback1.json

## Getting Started
These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites
Be sure to have installed the following on your machine.
- Docker
- MLflow
- Python 3.x
- scikit-learn

Run the folllowing to command to start the mlflow tracking UI on your local machine:
```
mlflow server --host=$MLFLOW_SERVER_HOST --port=$MLFLOW_SERVER_PORT --serve-artifacts
```
Be sure to use your IP address as the host server so that your docker containers can communicate with the MLflow tracking server. You can print your local IP with the following command:
```

```

### ML Workloads
We have defined three primary workfloads to be executed:
1. Parameter Tuning: running multiple mlflow runs in parallel and selecting the best result
2. Model Training: running model training
3. Deployment and Inference: serving the model to make predictions

### How to Run ML Workloads
1. Docker Containers (Recommended): run workloads as isolated snapshots of your code
2. Command-line: run the scripts directly from the console

### Getting Started
1. **Clone the Repository**
   ```sh
   git clone https://github.com/tristanoprofetto/ml-platform
   ```

2. **Define Environment Variables**
   Navigate to the automation folder and define the required environment variables in ./automation/variables.sh


3. **Run End-to-End ML Workflow**
   Start the containers using Docker Compose.
   ```sh
   bash ./automation/execute.sh
   ```

### Finding the best Model Parameters
1. Build the tuning image in ./dockerfile/tune by running:
```sh
docker build -t $TUNE_IMAGE_TAG -f ./dockerfiles/tune/Dockerfile .
```
2. Run the image as a container:
```sh
docker run -e MLFLOW_TRACKING_URI=$MLFLOW_TRACKING_URI $TUNE_IMAGE_URI
```
3. Check and Compare MLFlow runs through the Tracking UI

### Training the Model
1. Access the training container's shell.
2. Run the training script.
   ```sh
   python train.py
   ```

# Code Structure
Here is a quick breakdown of the code structure
* automation: automate the execution of ML workflows with bash scripts
* data: sample dataset for running training and tuning
* dockerfiles: required dockerfiles for building the images
* exceptions: custom defined exceptions for better error handling
* logger: custom Python logger
* steps: the required modules that execute our machine learning workflows
* tests: functional testing
* app.py: model server file

### Serving the Model

1. The serving container starts automatically with Docker Compose.
2. Access the model's predictions at `http://localhost:5000`.

## Usage

Provide detailed instructions on how to use the platform, including example requests and responses for the model serving API.

